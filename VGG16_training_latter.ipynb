{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, MaxPool2D , Flatten\n",
    "from keras.applications import VGG16\n",
    "from keras.applications.vgg16 import preprocess_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 4\n",
    "image_size = 224\n",
    "batch_size_training = 10\n",
    "batch_size_validation = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_generator = ImageDataGenerator(preprocessing_function=preprocess_input,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400 images belonging to 4 classes.\n",
      "Found 9 images belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = data_generator.flow_from_directory(\n",
    "    'New folder/train',\n",
    "    target_size = (image_size, image_size),\n",
    "    batch_size = batch_size_training,\n",
    "    class_mode = 'categorical'\n",
    "    )\n",
    "\n",
    "validation_generator = data_generator.flow_from_directory(\n",
    "    'New folder/validation',\n",
    "    target_size = (image_size, image_size),\n",
    "    batch_size = batch_size_training,\n",
    "    class_mode = 'categorical'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defingin the model\n",
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'int' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32md:\\Vizuamatix\\Neural Network\\VGG16_training_latter.ipynb Cell 6\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Vizuamatix/Neural%20Network/VGG16_training_latter.ipynb#W5sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m#adding VGG16 model\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Vizuamatix/Neural%20Network/VGG16_training_latter.ipynb#W5sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mxml\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39metree\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mElementInclude\u001b[39;00m \u001b[39mimport\u001b[39;00m include\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Vizuamatix/Neural%20Network/VGG16_training_latter.ipynb#W5sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m model\u001b[39m.\u001b[39madd(\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Vizuamatix/Neural%20Network/VGG16_training_latter.ipynb#W5sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     VGG16(include_top \u001b[39m=\u001b[39;49m \u001b[39mFalse\u001b[39;49;00m, pooling\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mavg\u001b[39;49m\u001b[39m'\u001b[39;49m, weights \u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mimagenet\u001b[39;49m\u001b[39m'\u001b[39;49m, input_shape\u001b[39m=\u001b[39;49m \u001b[39m224\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Vizuamatix/Neural%20Network/VGG16_training_latter.ipynb#W5sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\anaconda3\\lib\\site-packages\\keras\\applications\\vgg16.py:128\u001b[0m, in \u001b[0;36mVGG16\u001b[1;34m(include_top, weights, input_tensor, input_shape, pooling, classes, classifier_activation)\u001b[0m\n\u001b[0;32m    124\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mIf using `weights` as `\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mimagenet\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m` with `include_top` \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    125\u001b[0m                    \u001b[39m'\u001b[39m\u001b[39mas true, `classes` should be 1000.  \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    126\u001b[0m                    \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mReceived `classes=\u001b[39m\u001b[39m{\u001b[39;00mclasses\u001b[39m}\u001b[39;00m\u001b[39m`\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m    127\u001b[0m \u001b[39m# Determine proper input shape\u001b[39;00m\n\u001b[1;32m--> 128\u001b[0m input_shape \u001b[39m=\u001b[39m imagenet_utils\u001b[39m.\u001b[39;49mobtain_input_shape(\n\u001b[0;32m    129\u001b[0m     input_shape,\n\u001b[0;32m    130\u001b[0m     default_size\u001b[39m=\u001b[39;49m\u001b[39m224\u001b[39;49m,\n\u001b[0;32m    131\u001b[0m     min_size\u001b[39m=\u001b[39;49m\u001b[39m32\u001b[39;49m,\n\u001b[0;32m    132\u001b[0m     data_format\u001b[39m=\u001b[39;49mbackend\u001b[39m.\u001b[39;49mimage_data_format(),\n\u001b[0;32m    133\u001b[0m     require_flatten\u001b[39m=\u001b[39;49minclude_top,\n\u001b[0;32m    134\u001b[0m     weights\u001b[39m=\u001b[39;49mweights)\n\u001b[0;32m    136\u001b[0m \u001b[39mif\u001b[39;00m input_tensor \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    137\u001b[0m   img_input \u001b[39m=\u001b[39m layers\u001b[39m.\u001b[39mInput(shape\u001b[39m=\u001b[39minput_shape)\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\anaconda3\\lib\\site-packages\\keras\\applications\\imagenet_utils.py:369\u001b[0m, in \u001b[0;36mobtain_input_shape\u001b[1;34m(input_shape, default_size, min_size, data_format, require_flatten, weights)\u001b[0m\n\u001b[0;32m    367\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    368\u001b[0m   \u001b[39mif\u001b[39;00m input_shape \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 369\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39;49m(input_shape) \u001b[39m!=\u001b[39m \u001b[39m3\u001b[39m:\n\u001b[0;32m    370\u001b[0m       \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39m`input_shape` must be a tuple of three integers.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m    371\u001b[0m     \u001b[39mif\u001b[39;00m input_shape[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m!=\u001b[39m \u001b[39m3\u001b[39m \u001b[39mand\u001b[39;00m weights \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mimagenet\u001b[39m\u001b[39m'\u001b[39m:\n",
      "\u001b[1;31mTypeError\u001b[0m: object of type 'int' has no len()"
     ]
    }
   ],
   "source": [
    "#adding VGG16 model\n",
    "from xml.etree.ElementInclude import include\n",
    "\n",
    "\n",
    "model.add(\n",
    "    VGG16(include_top = False, pooling='avg', weights ='imagenet', input_shape= 224)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.engine.functional.Functional at 0x26bd164cd00>]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.engine.input_layer.InputLayer at 0x26bd1377fa0>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x26bd14fa280>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x26bd14fa4f0>,\n",
       " <keras.layers.pooling.max_pooling2d.MaxPooling2D at 0x26bd14faeb0>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x26bd14fa7f0>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x26bd16130a0>,\n",
       " <keras.layers.pooling.max_pooling2d.MaxPooling2D at 0x26bd161ebb0>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x26bd161e880>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x26bd1613ca0>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x26bd1624f70>,\n",
       " <keras.layers.pooling.max_pooling2d.MaxPooling2D at 0x26bd1613700>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x26bd1624be0>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x26bd1629880>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x26bd1634cd0>,\n",
       " <keras.layers.pooling.max_pooling2d.MaxPooling2D at 0x26bd16131f0>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x26bd1634c70>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x26bd1640460>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x26bd1640850>,\n",
       " <keras.layers.pooling.max_pooling2d.MaxPooling2D at 0x26bd16294c0>,\n",
       " <keras.layers.pooling.global_average_pooling2d.GlobalAveragePooling2D at 0x26bd163a520>]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[0].layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Flatten())\n",
    "model.add(Dense(units=4096,activation=\"relu\"))\n",
    "model.add(Dense(units=4096,activation=\"relu\"))\n",
    "model.add(Dense(units=4, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.engine.functional.Functional at 0x26bd164cd00>,\n",
       " <keras.layers.reshaping.flatten.Flatten at 0x26bd139c6a0>,\n",
       " <keras.layers.core.dense.Dense at 0x26bd1683d00>,\n",
       " <keras.layers.core.dense.Dense at 0x26bd167bac0>,\n",
       " <keras.layers.core.dense.Dense at 0x26bc7412c10>]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras.engine.functional.Functional object at 0x0000026BD164CD00> True\n",
      "<keras.layers.reshaping.flatten.Flatten object at 0x0000026BD139C6A0> True\n",
      "<keras.layers.core.dense.Dense object at 0x0000026BD1683D00> True\n",
      "<keras.layers.core.dense.Dense object at 0x0000026BD167BAC0> True\n",
      "<keras.layers.core.dense.Dense object at 0x0000026BC7412C10> True\n"
     ]
    }
   ],
   "source": [
    "for layer in model.layers:\n",
    "\n",
    "    print(layer, layer.trainable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training the model\n",
    "model.layers[0].trainable = False\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics =['accuracy'])\n",
    "steps_per_epoch_train = len(train_generator)\n",
    "steps_per_epoch_validation = len(validation_generator)\n",
    "number_epochs = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras.engine.functional.Functional object at 0x0000026BD164CD00> False\n",
      "<keras.layers.reshaping.flatten.Flatten object at 0x0000026BD139C6A0> True\n",
      "<keras.layers.core.dense.Dense object at 0x0000026BD1683D00> True\n",
      "<keras.layers.core.dense.Dense object at 0x0000026BD167BAC0> True\n",
      "<keras.layers.core.dense.Dense object at 0x0000026BC7412C10> True\n"
     ]
    }
   ],
   "source": [
    "for layer in model.layers:\n",
    "\n",
    "    print(layer, layer.trainable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " vgg16 (Functional)          (None, 512)               14714688  \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 512)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 4096)              2101248   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 4096)              16781312  \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 4)                 16388     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 33,613,636\n",
      "Trainable params: 18,898,948\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_20788\\2980021967.py:1: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  fit_history = model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch_train, epochs=number_epochs,verbose=1, validation_data=validation_generator, validation_steps=steps_per_epoch_validation,)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - 40s 978ms/step - loss: 2.7418 - accuracy: 0.8950 - val_loss: 0.0049 - val_accuracy: 1.0000\n",
      "Epoch 2/2\n",
      "40/40 [==============================] - 45s 1s/step - loss: 1.4590e-06 - accuracy: 1.0000 - val_loss: 2.8875e-06 - val_accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "fit_history = model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch_train, epochs=number_epochs,verbose=1, validation_data=validation_generator, validation_steps=steps_per_epoch_validation,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model/VGG16_latter_training/vgg16_Product_Classification1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "vgg16_saved = load_model('model/VGG16/vgg16_Product_Classification_2Dense.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "basemodel = Model(inputs=vgg16_saved.input, outputs=vgg16_saved.get_layer('dense').output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 355ms/step\n",
      "1/1 [==============================] - 0s 166ms/step\n",
      "Cosine Similarity: 0.9403707\n"
     ]
    }
   ],
   "source": [
    "#predicting\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "# load an image from file\n",
    "image1 = load_img('New folder/test/test4.jpg', target_size=(224, 224))\n",
    "image2 = load_img('New folder/test/test3.png', target_size=(224, 224))\n",
    "\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "image1 = img_to_array(image1)\n",
    "image2 = img_to_array(image2)\n",
    "\n",
    "# reshape data for the model\n",
    "image1 = image1.reshape((1, image1.shape[0], image1.shape[1], image1.shape[2]))\n",
    "image2 = image2.reshape((1, image2.shape[0], image2.shape[1], image2.shape[2]))\n",
    "\n",
    "\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "# prepare the image for the VGG model\n",
    "image1 = preprocess_input(image1)\n",
    "image2 = preprocess_input(image2)\n",
    "\n",
    "# predict the probability across all output classes\n",
    "yhat1_base = basemodel.predict(image1)\n",
    "yhat2_base = basemodel.predict(image2)\n",
    "\n",
    "#cosine similarity\n",
    "A = yhat1_base[0]\n",
    "B = yhat2_base[0]\n",
    "\n",
    "cosine = np.dot(A,B)/(norm(A)*norm(B))\n",
    "print(\"Cosine Similarity:\", cosine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 133ms/step\n",
      "1/1 [==============================] - 0s 134ms/step\n",
      "Cosine Similarity: 0.83852446\n"
     ]
    }
   ],
   "source": [
    "#predicting\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "# load an image from file\n",
    "image1 = load_img('New folder/test/test4.jpg', target_size=(224, 224))\n",
    "image2 = load_img('New folder/test/test.png', target_size=(224, 224))\n",
    "\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "image1 = img_to_array(image1)\n",
    "image2 = img_to_array(image2)\n",
    "\n",
    "# reshape data for the model\n",
    "image1 = image1.reshape((1, image1.shape[0], image1.shape[1], image1.shape[2]))\n",
    "image2 = image2.reshape((1, image2.shape[0], image2.shape[1], image2.shape[2]))\n",
    "\n",
    "\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "# prepare the image for the VGG model\n",
    "image1 = preprocess_input(image1)\n",
    "image2 = preprocess_input(image2)\n",
    "\n",
    "# predict the probability across all output classes\n",
    "yhat1_base = basemodel.predict(image1)\n",
    "yhat2_base = basemodel.predict(image2)\n",
    "\n",
    "#cosine similarity\n",
    "A = yhat1_base[0]\n",
    "B = yhat2_base[0]\n",
    "\n",
    "cosine = np.dot(A,B)/(norm(A)*norm(B))\n",
    "print(\"Cosine Similarity:\", cosine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 129ms/step\n",
      "1/1 [==============================] - 0s 137ms/step\n",
      "Cosine Similarity: 0.87827444\n"
     ]
    }
   ],
   "source": [
    "#predicting\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "# load an image from file\n",
    "image1 = load_img('New folder/test/test.png', target_size=(224, 224))\n",
    "image2 = load_img('New folder/test/test3.png', target_size=(224, 224))\n",
    "\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "image1 = img_to_array(image1)\n",
    "image2 = img_to_array(image2)\n",
    "\n",
    "# reshape data for the model\n",
    "image1 = image1.reshape((1, image1.shape[0], image1.shape[1], image1.shape[2]))\n",
    "image2 = image2.reshape((1, image2.shape[0], image2.shape[1], image2.shape[2]))\n",
    "\n",
    "\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "# prepare the image for the VGG model\n",
    "image1 = preprocess_input(image1)\n",
    "image2 = preprocess_input(image2)\n",
    "\n",
    "# predict the probability across all output classes\n",
    "yhat1_base = basemodel.predict(image1)\n",
    "yhat2_base = basemodel.predict(image2)\n",
    "\n",
    "#cosine similarity\n",
    "A = yhat1_base[0]\n",
    "B = yhat2_base[0]\n",
    "\n",
    "cosine = np.dot(A,B)/(norm(A)*norm(B))\n",
    "print(\"Cosine Similarity:\", cosine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a90aeebcf29d64a654773811cc170cb25061cb2498f10ac689db374c7bf325de"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
