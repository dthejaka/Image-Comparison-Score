{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras,os\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, MaxPool2D , Flatten\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import zipfile \n",
    "import tensorflow as tf \n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator \n",
    "from tensorflow.keras import layers \n",
    "from tensorflow.keras import Model \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = 'D:/Vizuamatix/Neural Network/New folder'\n",
    "train_dir = os.path.join(base_dir, 'train')\n",
    "validation_dir = os.path.join(base_dir, 'test')\n",
    "\n",
    "# Directory with our training cat pictures\n",
    "train_Dove_Anti_dir = os.path.join(train_dir, 'Dove Anti Dandruff Shampoo, 180ml')\n",
    "\n",
    "# Directory with our training dog pictures\n",
    "train_Dove_Intense_dir = os.path.join(train_dir, 'Dove Intense Repair Shampoo, 180ml')\n",
    "\n",
    "# Directory with our training dog pictures\n",
    "train_Lifebuoy_dir = os.path.join(train_dir, 'Lifebuoy Kitchen Fresh Handwash, 200ml')\n",
    "\n",
    "# Directory with our training dog pictures\n",
    "train_Sunsilk_dir = os.path.join(train_dir, 'sunsilk_detox_180ml')\n",
    "\n",
    "validation_Dove_Anti_dir = os.path.join(train_dir, 'Dove Anti Dandruff Shampoo, 180ml')\n",
    "\n",
    "# Directory with our training dog pictures\n",
    "validation_Dove_Intense_dir = os.path.join(train_dir, 'Dove Intense Repair Shampoo, 180ml')\n",
    "\n",
    "# Directory with our training dog pictures\n",
    "validation_Lifebuoy_dir = os.path.join(train_dir, 'Lifebuoy Kitchen Fresh Handwash, 200ml')\n",
    "\n",
    "# Directory with our training dog pictures\n",
    "validation_Sunsilk_dir = os.path.join(train_dir, 'sunsilk_detox_180ml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_cats_dir' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32md:\\Vizuamatix\\Neural Network\\trial1.ipynb Cell 4\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Vizuamatix/Neural%20Network/trial1.ipynb#W3sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m fig\u001b[39m.\u001b[39mset_size_inches(ncols\u001b[39m*\u001b[39m\u001b[39m4\u001b[39m, nrows\u001b[39m*\u001b[39m\u001b[39m4\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Vizuamatix/Neural%20Network/trial1.ipynb#W3sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m pic_index \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Vizuamatix/Neural%20Network/trial1.ipynb#W3sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m train_cat_fnames \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mlistdir( train_cats_dir )\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Vizuamatix/Neural%20Network/trial1.ipynb#W3sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m train_dog_fnames \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mlistdir( train_dogs_dir )\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Vizuamatix/Neural%20Network/trial1.ipynb#W3sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m next_cat_pix \u001b[39m=\u001b[39m [os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(train_cats_dir, fname) \n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Vizuamatix/Neural%20Network/trial1.ipynb#W3sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m                 \u001b[39mfor\u001b[39;00m fname \u001b[39min\u001b[39;00m train_cat_fnames[ pic_index\u001b[39m-\u001b[39m\u001b[39m8\u001b[39m:pic_index] \n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Vizuamatix/Neural%20Network/trial1.ipynb#W3sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m                ]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_cats_dir' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1152x1152 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Set up matplotlib fig, and size it to fit 4x4 pics\n",
    "import matplotlib.image as mpimg\n",
    "nrows = 4\n",
    "ncols = 4\n",
    "\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(ncols*4, nrows*4)\n",
    "pic_index = 1\n",
    "train_cat_fnames = os.listdir( train_cats_dir )\n",
    "train_dog_fnames = os.listdir( train_dogs_dir )\n",
    "\n",
    "\n",
    "next_cat_pix = [os.path.join(train_cats_dir, fname) \n",
    "                for fname in train_cat_fnames[ pic_index-8:pic_index] \n",
    "               ]\n",
    "\n",
    "next_dog_pix = [os.path.join(train_dogs_dir, fname) \n",
    "                for fname in train_dog_fnames[ pic_index-8:pic_index]\n",
    "               ]\n",
    "\n",
    "for i, img_path in enumerate(next_cat_pix+next_dog_pix):\n",
    "  # Set up subplot; subplot indices start at 1\n",
    "  sp = plt.subplot(nrows, ncols, i + 1)\n",
    "  sp.axis('Off') # Don't show axes (or gridlines)\n",
    "\n",
    "  img = mpimg.imread(img_path)\n",
    "  plt.imshow(img)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(rescale = 1./255.,rotation_range = 40, width_shift_range = 0.2, height_shift_range = 0.2, shear_range = 0.2, zoom_range = 0.2, horizontal_flip = True)\n",
    "\n",
    "# Note that the validation data should not be augmented!\n",
    "test_datagen = ImageDataGenerator( rescale = 1.0/255. )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 232 images belonging to 4 classes.\n",
      "Found 9 images belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = train_datagen.flow_from_directory(train_dir, batch_size = 20, class_mode = 'binary', target_size = (224, 224))\n",
    "\n",
    " #Flow validation images in batches of 20 using test_datagen generator\n",
    "validation_generator = test_datagen.flow_from_directory( validation_dir,  batch_size = 20, class_mode = 'binary', target_size = (224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "\n",
    "base_model = VGG16(input_shape = (224, 224, 3), # Shape of our images\n",
    "include_top = False, # Leave out the last fully connected layer\n",
    "weights = 'imagenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in base_model.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\optimizers\\optimizer_v2\\rmsprop.py:135: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(RMSprop, self).__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Flatten the output layer to 1 dimension\n",
    "x = layers.Flatten()(base_model.output)\n",
    "\n",
    "# Add a fully connected layer with 512 hidden units and ReLU activation\n",
    "x = layers.Dense(512, activation='relu')(x)\n",
    "\n",
    "# Add a dropout rate of 0.5\n",
    "x = layers.Dropout(0.5)(x)\n",
    "\n",
    "# Add a final sigmoid layer with 1 node for classification output\n",
    "x = layers.Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model = tf.keras.models.Model(base_model.input, x)\n",
    "\n",
    "model.compile(optimizer = tf.keras.optimizers.RMSprop(lr=0.0001), loss = 'binary_crossentropy',metrics = ['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "10/10 [==============================] - 23s 2s/step - loss: -36.9222 - acc: 0.2083 - val_loss: -35.9616 - val_acc: 0.3333\n",
      "Epoch 2/10\n",
      "10/10 [==============================] - 23s 2s/step - loss: -50.7248 - acc: 0.2396 - val_loss: -48.2148 - val_acc: 0.3333\n",
      "Epoch 3/10\n",
      "10/10 [==============================] - 26s 3s/step - loss: -69.4569 - acc: 0.2188 - val_loss: -61.8796 - val_acc: 0.3333\n",
      "Epoch 4/10\n",
      "10/10 [==============================] - 24s 2s/step - loss: -84.6455 - acc: 0.2188 - val_loss: -75.7354 - val_acc: 0.3333\n",
      "Epoch 5/10\n",
      "10/10 [==============================] - 25s 2s/step - loss: -100.2563 - acc: 0.2188 - val_loss: -88.8738 - val_acc: 0.3333\n",
      "Epoch 6/10\n",
      "10/10 [==============================] - 25s 2s/step - loss: -116.0669 - acc: 0.1927 - val_loss: -103.0027 - val_acc: 0.3333\n",
      "Epoch 7/10\n",
      "10/10 [==============================] - 26s 3s/step - loss: -143.9135 - acc: 0.2188 - val_loss: -118.1576 - val_acc: 0.3333\n",
      "Epoch 8/10\n",
      "10/10 [==============================] - 25s 2s/step - loss: -154.1003 - acc: 0.2240 - val_loss: -132.8884 - val_acc: 0.3333\n",
      "Epoch 9/10\n",
      "10/10 [==============================] - 25s 3s/step - loss: -163.6196 - acc: 0.2292 - val_loss: -148.1044 - val_acc: 0.3333\n",
      "Epoch 10/10\n",
      "10/10 [==============================] - 24s 2s/step - loss: -195.5533 - acc: 0.2188 - val_loss: -162.8663 - val_acc: 0.3333\n"
     ]
    }
   ],
   "source": [
    "vgghist = model.fit(train_generator, validation_data = validation_generator, steps_per_epoch = 10, epochs = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2e1ebdc1510>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vgghist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "# load an image from file\n",
    "image = load_img('test.png', target_size=(224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "# convert the image pixels to a numpy array\n",
    "image = img_to_array(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape data for the model\n",
    "image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.vgg16 import preprocess_input\n",
    "# prepare the image for the VGG model\n",
    "image = preprocess_input(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels.h5\n",
      " 78217216/553467096 [===>..........................] - ETA: 11:15"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\Vizuamatix\\Neural Network\\trial1.ipynb Cell 16\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Vizuamatix/Neural%20Network/trial1.ipynb#X32sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m model \u001b[39m=\u001b[39m VGG16()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\applications\\vgg16.py:212\u001b[0m, in \u001b[0;36mVGG16\u001b[1;34m(include_top, weights, input_tensor, input_shape, pooling, classes, classifier_activation)\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[39mif\u001b[39;00m weights \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mimagenet\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m    211\u001b[0m   \u001b[39mif\u001b[39;00m include_top:\n\u001b[1;32m--> 212\u001b[0m     weights_path \u001b[39m=\u001b[39m data_utils\u001b[39m.\u001b[39;49mget_file(\n\u001b[0;32m    213\u001b[0m         \u001b[39m'\u001b[39;49m\u001b[39mvgg16_weights_tf_dim_ordering_tf_kernels.h5\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[0;32m    214\u001b[0m         WEIGHTS_PATH,\n\u001b[0;32m    215\u001b[0m         cache_subdir\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mmodels\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[0;32m    216\u001b[0m         file_hash\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m64373286793e3c8b2b4e3219cbf3544b\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m    217\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    218\u001b[0m     weights_path \u001b[39m=\u001b[39m data_utils\u001b[39m.\u001b[39mget_file(\n\u001b[0;32m    219\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mvgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m    220\u001b[0m         WEIGHTS_PATH_NO_TOP,\n\u001b[0;32m    221\u001b[0m         cache_subdir\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmodels\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m    222\u001b[0m         file_hash\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m6d6bbae143d832006294945121d1f1fc\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\utils\\data_utils.py:283\u001b[0m, in \u001b[0;36mget_file\u001b[1;34m(fname, origin, untar, md5_hash, file_hash, cache_subdir, hash_algorithm, extract, archive_format, cache_dir)\u001b[0m\n\u001b[0;32m    281\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    282\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 283\u001b[0m     urlretrieve(origin, fpath, DLProgbar())\n\u001b[0;32m    284\u001b[0m   \u001b[39mexcept\u001b[39;00m urllib\u001b[39m.\u001b[39merror\u001b[39m.\u001b[39mHTTPError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    285\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mException\u001b[39;00m(error_msg\u001b[39m.\u001b[39mformat(origin, e\u001b[39m.\u001b[39mcode, e\u001b[39m.\u001b[39mmsg))\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\utils\\data_utils.py:84\u001b[0m, in \u001b[0;36murlretrieve\u001b[1;34m(url, filename, reporthook, data)\u001b[0m\n\u001b[0;32m     82\u001b[0m response \u001b[39m=\u001b[39m urlopen(url, data)\n\u001b[0;32m     83\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(filename, \u001b[39m'\u001b[39m\u001b[39mwb\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m fd:\n\u001b[1;32m---> 84\u001b[0m   \u001b[39mfor\u001b[39;00m chunk \u001b[39min\u001b[39;00m chunk_read(response, reporthook\u001b[39m=\u001b[39mreporthook):\n\u001b[0;32m     85\u001b[0m     fd\u001b[39m.\u001b[39mwrite(chunk)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\utils\\data_utils.py:73\u001b[0m, in \u001b[0;36murlretrieve.<locals>.chunk_read\u001b[1;34m(response, chunk_size, reporthook)\u001b[0m\n\u001b[0;32m     71\u001b[0m count \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m     72\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m---> 73\u001b[0m   chunk \u001b[39m=\u001b[39m response\u001b[39m.\u001b[39;49mread(chunk_size)\n\u001b[0;32m     74\u001b[0m   count \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m     75\u001b[0m   \u001b[39mif\u001b[39;00m reporthook \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_3.10.1776.0_x64__qbz5n2kfra8p0\\lib\\http\\client.py:465\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    462\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlength \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m amt \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlength:\n\u001b[0;32m    463\u001b[0m     \u001b[39m# clip the read to the \"end of response\"\u001b[39;00m\n\u001b[0;32m    464\u001b[0m     amt \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlength\n\u001b[1;32m--> 465\u001b[0m s \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfp\u001b[39m.\u001b[39;49mread(amt)\n\u001b[0;32m    466\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m s \u001b[39mand\u001b[39;00m amt:\n\u001b[0;32m    467\u001b[0m     \u001b[39m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[0;32m    468\u001b[0m     \u001b[39m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[0;32m    469\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_close_conn()\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_3.10.1776.0_x64__qbz5n2kfra8p0\\lib\\socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    703\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m    704\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 705\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[0;32m    706\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n\u001b[0;32m    707\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_3.10.1776.0_x64__qbz5n2kfra8p0\\lib\\ssl.py:1274\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1270\u001b[0m     \u001b[39mif\u001b[39;00m flags \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m   1271\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1272\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[0;32m   1273\u001b[0m           \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m)\n\u001b[1;32m-> 1274\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(nbytes, buffer)\n\u001b[0;32m   1275\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1276\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_3.10.1776.0_x64__qbz5n2kfra8p0\\lib\\ssl.py:1130\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1128\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1129\u001b[0m     \u001b[39mif\u001b[39;00m buffer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1130\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sslobj\u001b[39m.\u001b[39;49mread(\u001b[39mlen\u001b[39;49m, buffer)\n\u001b[0;32m   1131\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1132\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sslobj\u001b[39m.\u001b[39mread(\u001b[39mlen\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = VGG16()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'History' object has no attribute 'predict'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32md:\\Vizuamatix\\Neural Network\\trial1.ipynb Cell 16\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Vizuamatix/Neural%20Network/trial1.ipynb#X26sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# predict the probability across all output classes\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Vizuamatix/Neural%20Network/trial1.ipynb#X26sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m yhat \u001b[39m=\u001b[39m vgghist\u001b[39m.\u001b[39;49mpredict(image)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'History' object has no attribute 'predict'"
     ]
    }
   ],
   "source": [
    "# predict the probability across all output classes\n",
    "yhat = vgghist.predict(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "`decode_predictions` expects a batch of predictions (i.e. a 2D array of shape (samples, 1000)). Found array with shape: (1, 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32md:\\Vizuamatix\\Neural Network\\trial1.ipynb Cell 17\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Vizuamatix/Neural%20Network/trial1.ipynb#X30sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mapplications\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mvgg16\u001b[39;00m \u001b[39mimport\u001b[39;00m decode_predictions\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Vizuamatix/Neural%20Network/trial1.ipynb#X30sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m# convert the probabilities to class labels\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Vizuamatix/Neural%20Network/trial1.ipynb#X30sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m label \u001b[39m=\u001b[39m decode_predictions(yhat)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Vizuamatix/Neural%20Network/trial1.ipynb#X30sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# retrieve the most likely result, e.g. highest probability\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Vizuamatix/Neural%20Network/trial1.ipynb#X30sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m label \u001b[39m=\u001b[39m label[\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\applications\\vgg16.py:238\u001b[0m, in \u001b[0;36mdecode_predictions\u001b[1;34m(preds, top)\u001b[0m\n\u001b[0;32m    236\u001b[0m \u001b[39m@keras_export\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mkeras.applications.vgg16.decode_predictions\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m    237\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecode_predictions\u001b[39m(preds, top\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m):\n\u001b[1;32m--> 238\u001b[0m   \u001b[39mreturn\u001b[39;00m imagenet_utils\u001b[39m.\u001b[39;49mdecode_predictions(preds, top\u001b[39m=\u001b[39;49mtop)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\applications\\imagenet_utils.py:147\u001b[0m, in \u001b[0;36mdecode_predictions\u001b[1;34m(preds, top)\u001b[0m\n\u001b[0;32m    144\u001b[0m \u001b[39mglobal\u001b[39;00m CLASS_INDEX\n\u001b[0;32m    146\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(preds\u001b[39m.\u001b[39mshape) \u001b[39m!=\u001b[39m \u001b[39m2\u001b[39m \u001b[39mor\u001b[39;00m preds\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m] \u001b[39m!=\u001b[39m \u001b[39m1000\u001b[39m:\n\u001b[1;32m--> 147\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39m`decode_predictions` expects \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    148\u001b[0m                    \u001b[39m'\u001b[39m\u001b[39ma batch of predictions \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    149\u001b[0m                    \u001b[39m'\u001b[39m\u001b[39m(i.e. a 2D array of shape (samples, 1000)). \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    150\u001b[0m                    \u001b[39m'\u001b[39m\u001b[39mFound array with shape: \u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(preds\u001b[39m.\u001b[39mshape))\n\u001b[0;32m    151\u001b[0m \u001b[39mif\u001b[39;00m CLASS_INDEX \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    152\u001b[0m   fpath \u001b[39m=\u001b[39m data_utils\u001b[39m.\u001b[39mget_file(\n\u001b[0;32m    153\u001b[0m       \u001b[39m'\u001b[39m\u001b[39mimagenet_class_index.json\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m    154\u001b[0m       CLASS_INDEX_PATH,\n\u001b[0;32m    155\u001b[0m       cache_subdir\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmodels\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m    156\u001b[0m       file_hash\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mc2c37ea517e94d9795004a39431a14cb\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: `decode_predictions` expects a batch of predictions (i.e. a 2D array of shape (samples, 1000)). Found array with shape: (1, 1)"
     ]
    }
   ],
   "source": [
    "from keras.applications.vgg16 import decode_predictions\n",
    "# convert the probabilities to class labels\n",
    "label = decode_predictions(yhat)\n",
    "# retrieve the most likely result, e.g. highest probability\n",
    "label = label[0][0]\n",
    "# print the classification\n",
    "print('%s (%.2f%%)' % (label[1], label[2]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "95cdb06e919eab5e2c554174537356ac9b55200d1eb6f880dc25de04343a18ea"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
