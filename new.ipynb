{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.applications import VGG16\n",
    "from keras.applications.vgg16 import preprocess_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 4\n",
    "image_size = 224\n",
    "batch_size_training = 10\n",
    "batch_size_validation = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_generator = ImageDataGenerator(preprocessing_function=preprocess_input,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 232 images belonging to 4 classes.\n",
      "Found 9 images belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = data_generator.flow_from_directory(\n",
    "    'New folder/train',\n",
    "    target_size = (image_size, image_size),\n",
    "    batch_size = batch_size_training,\n",
    "    class_mode = 'categorical'\n",
    "    )\n",
    "\n",
    "validation_generator = data_generator.flow_from_directory(\n",
    "    'New folder/validation',\n",
    "    target_size = (image_size, image_size),\n",
    "    batch_size = batch_size_training,\n",
    "    class_mode = 'categorical'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defingin the model\n",
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding VGG16 model\n",
    "from xml.etree.ElementInclude import include\n",
    "\n",
    "\n",
    "model.add(\n",
    "    VGG16(include_top = False, pooling='avg', weights ='imagenet')\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.engine.functional.Functional at 0x163787d3d00>,\n",
       " <keras.layers.core.dense.Dense at 0x1630002b730>,\n",
       " <keras.layers.core.dense.Dense at 0x1630002bc40>]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.add(Dense(units=4096,activation=\"relu\"))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " vgg16 (Functional)          (None, 512)               14714688  \n",
      "                                                                 \n",
      " dense (Dense)               (None, 4096)              2101248   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 4)                 16388     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 16,832,324\n",
      "Trainable params: 16,832,324\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.layers.core.dense.Dense at 0x1630002b730>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#number of layers\n",
    "model.layers[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training the model\n",
    "model.layers[0].trainable = False\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics =['accuracy'])\n",
    "steps_per_epoch_train = len(train_generator)\n",
    "steps_per_epoch_validation = len(validation_generator)\n",
    "number_epochs = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "steps_per_epoch_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "steps_per_epoch_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras.engine.input_layer.InputLayer object at 0x000001637854DA30> False\n",
      "<keras.layers.convolutional.conv2d.Conv2D object at 0x000001637869F2B0> False\n",
      "<keras.layers.convolutional.conv2d.Conv2D object at 0x000001637869F520> False\n",
      "<keras.layers.pooling.max_pooling2d.MaxPooling2D object at 0x000001637869FEB0> False\n",
      "<keras.layers.convolutional.conv2d.Conv2D object at 0x000001637869F820> False\n",
      "<keras.layers.convolutional.conv2d.Conv2D object at 0x000001637879A0A0> False\n",
      "<keras.layers.pooling.max_pooling2d.MaxPooling2D object at 0x00000163787A7BB0> False\n",
      "<keras.layers.convolutional.conv2d.Conv2D object at 0x00000163787A7880> False\n",
      "<keras.layers.convolutional.conv2d.Conv2D object at 0x000001637879ACA0> False\n",
      "<keras.layers.convolutional.conv2d.Conv2D object at 0x00000163787ACF70> False\n",
      "<keras.layers.pooling.max_pooling2d.MaxPooling2D object at 0x000001637879A700> False\n",
      "<keras.layers.convolutional.conv2d.Conv2D object at 0x00000163787ACBE0> False\n",
      "<keras.layers.convolutional.conv2d.Conv2D object at 0x00000163787B0880> False\n",
      "<keras.layers.convolutional.conv2d.Conv2D object at 0x00000163787BDCD0> False\n",
      "<keras.layers.pooling.max_pooling2d.MaxPooling2D object at 0x000001637879A1F0> False\n",
      "<keras.layers.convolutional.conv2d.Conv2D object at 0x00000163787BDC70> False\n",
      "<keras.layers.convolutional.conv2d.Conv2D object at 0x00000163787C9460> False\n",
      "<keras.layers.convolutional.conv2d.Conv2D object at 0x00000163787C9850> False\n",
      "<keras.layers.pooling.max_pooling2d.MaxPooling2D object at 0x00000163787B04C0> False\n",
      "<keras.layers.pooling.global_average_pooling2d.GlobalAveragePooling2D object at 0x00000163787C2550> False\n"
     ]
    }
   ],
   "source": [
    "for layer in model.layers[0].layers:\n",
    "\n",
    "    print(layer, layer.trainable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_13028\\2980021967.py:1: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  fit_history = model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch_train, epochs=number_epochs,verbose=1, validation_data=validation_generator, validation_steps=steps_per_epoch_validation,)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "24/24 [==============================] - 37s 1s/step - loss: 1.1426 - accuracy: 0.8534 - val_loss: 0.2574 - val_accuracy: 0.7778\n",
      "Epoch 2/2\n",
      "24/24 [==============================] - 34s 1s/step - loss: 0.0174 - accuracy: 0.9957 - val_loss: 0.0792 - val_accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "fit_history = model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch_train, epochs=number_epochs,verbose=1, validation_data=validation_generator, validation_steps=steps_per_epoch_validation,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model/VGG16/vgg16_Product_Classification_2Dense.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32md:\\Vizuamatix\\Neural Network\\new.ipynb Cell 13\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Vizuamatix/Neural%20Network/new.ipynb#X13sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m#save the model\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Vizuamatix/Neural%20Network/new.ipynb#X13sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgoogle\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcolab\u001b[39;00m \u001b[39mimport\u001b[39;00m drive\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "#save the model\n",
    "from google.colab import drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "vgg16_saved = load_model('model/VGG16/vgg16_Product_Classification_2Dense.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.layers.core.dense.Dense at 0x17eef0e22e0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vgg16_saved.layers[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " vgg16 (Functional)          (None, 512)               14714688  \n",
      "                                                                 \n",
      " dense (Dense)               (None, 4096)              2101248   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 4)                 16388     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 16,832,324\n",
      "Trainable params: 2,117,636\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vgg16_saved.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 476ms/step\n"
     ]
    }
   ],
   "source": [
    "#predicting\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "# load an image from file\n",
    "image = load_img('New folder/test/test.png', target_size=(224, 224))\n",
    "\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "image = img_to_array(image)\n",
    "\n",
    "# reshape data for the model\n",
    "image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
    "\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "# prepare the image for the VGG model\n",
    "image = preprocess_input(image)\n",
    "\n",
    "# predict the probability across all output classes\n",
    "yhat = vgg16_saved.predict(image)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9.9999964e-01, 3.1711875e-07, 9.5229979e-33, 2.9516987e-17],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yhat[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'T-shirt/top'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "\n",
    "class_names[np.argmax(yhat)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 4)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yhat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "basemodel = Model(inputs=vgg16_saved.input, outputs=vgg16_saved.get_layer('dense').output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 193ms/step\n",
      "1/1 [==============================] - 0s 150ms/step\n"
     ]
    }
   ],
   "source": [
    "#predicting\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "# load an image from file\n",
    "image1 = load_img('New folder/test/test4.jpg', target_size=(224, 224))\n",
    "image2 = load_img('New folder/test/test3.png', target_size=(224, 224))\n",
    "\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "image1 = img_to_array(image1)\n",
    "image2 = img_to_array(image2)\n",
    "\n",
    "# reshape data for the model\n",
    "image1 = image1.reshape((1, image1.shape[0], image1.shape[1], image1.shape[2]))\n",
    "image2 = image2.reshape((1, image2.shape[0], image2.shape[1], image2.shape[2]))\n",
    "\n",
    "\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "# prepare the image for the VGG model\n",
    "image1 = preprocess_input(image1)\n",
    "image2 = preprocess_input(image2)\n",
    "\n",
    "# predict the probability across all output classes\n",
    "yhat1_base = basemodel.predict(image1)\n",
    "yhat2_base = basemodel.predict(image2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.9822604, 2.2466671, 0.       , ..., 0.       , 0.       ,\n",
       "        0.6798813]], dtype=float32)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yhat1_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity: 0.9403707\n"
     ]
    }
   ],
   "source": [
    "# import required libraries\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "\n",
    "A = yhat1_base[0]\n",
    "B = yhat2_base[0]\n",
    "\n",
    "cosine = np.dot(A,B)/(norm(A)*norm(B))\n",
    "print(\"Cosine Similarity:\", cosine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 139ms/step\n",
      "1/1 [==============================] - 0s 132ms/step\n"
     ]
    }
   ],
   "source": [
    "#predicting\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "# load an image from file\n",
    "image1 = load_img('New folder/test/test.png', target_size=(224, 224))\n",
    "image2 = load_img('New folder/test/test3.png', target_size=(224, 224))\n",
    "\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "image1 = img_to_array(image1)\n",
    "image2 = img_to_array(image2)\n",
    "\n",
    "# reshape data for the model\n",
    "image1 = image1.reshape((1, image1.shape[0], image1.shape[1], image1.shape[2]))\n",
    "image2 = image2.reshape((1, image2.shape[0], image2.shape[1], image2.shape[2]))\n",
    "\n",
    "\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "# prepare the image for the VGG model\n",
    "image1 = preprocess_input(image1)\n",
    "image2 = preprocess_input(image2)\n",
    "\n",
    "# predict the probability across all output classes\n",
    "yhat1_base = basemodel.predict(image1)\n",
    "yhat2_base = basemodel.predict(image2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity: 0.87827444\n"
     ]
    }
   ],
   "source": [
    "# import required libraries\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "\n",
    "A = yhat1_base[0]\n",
    "B = yhat2_base[0]\n",
    "\n",
    "cosine = np.dot(A,B)/(norm(A)*norm(B))\n",
    "print(\"Cosine Similarity:\", cosine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 125ms/step\n",
      "1/1 [==============================] - 0s 137ms/step\n"
     ]
    }
   ],
   "source": [
    "#predicting\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "# load an image from file\n",
    "image1 = load_img('New folder/test/test2.jpg', target_size=(224, 224))\n",
    "image2 = load_img('New folder/test/test3.png', target_size=(224, 224))\n",
    "\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "image1 = img_to_array(image1)\n",
    "image2 = img_to_array(image2)\n",
    "\n",
    "# reshape data for the model\n",
    "image1 = image1.reshape((1, image1.shape[0], image1.shape[1], image1.shape[2]))\n",
    "image2 = image2.reshape((1, image2.shape[0], image2.shape[1], image2.shape[2]))\n",
    "\n",
    "\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "# prepare the image for the VGG model\n",
    "image1 = preprocess_input(image1)\n",
    "image2 = preprocess_input(image2)\n",
    "\n",
    "# predict the probability across all output classes\n",
    "yhat1_base = basemodel.predict(image1)\n",
    "yhat2_base = basemodel.predict(image2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity: 0.47206917\n"
     ]
    }
   ],
   "source": [
    "A = yhat1_base[0]\n",
    "B = yhat2_base[0]\n",
    "\n",
    "cosine = np.dot(A,B)/(norm(A)*norm(B))\n",
    "print(\"Cosine Similarity:\", cosine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 131ms/step\n",
      "1/1 [==============================] - 0s 122ms/step\n"
     ]
    }
   ],
   "source": [
    "#predicting\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "# load an image from file\n",
    "image1 = load_img('New folder/test/test2.jpg', target_size=(224, 224))\n",
    "image2 = load_img('New folder/test/test5.png', target_size=(224, 224))\n",
    "\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "image1 = img_to_array(image1)\n",
    "image2 = img_to_array(image2)\n",
    "\n",
    "# reshape data for the model\n",
    "image1 = image1.reshape((1, image1.shape[0], image1.shape[1], image1.shape[2]))\n",
    "image2 = image2.reshape((1, image2.shape[0], image2.shape[1], image2.shape[2]))\n",
    "\n",
    "\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "# prepare the image for the VGG model\n",
    "image1 = preprocess_input(image1)\n",
    "image2 = preprocess_input(image2)\n",
    "\n",
    "# predict the probability across all output classes\n",
    "yhat1_base = basemodel.predict(image1)\n",
    "yhat2_base = basemodel.predict(image2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity: 0.5669977\n"
     ]
    }
   ],
   "source": [
    "A = yhat1_base[0]\n",
    "B = yhat2_base[0]\n",
    "\n",
    "cosine = np.dot(A,B)/(norm(A)*norm(B))\n",
    "print(\"Cosine Similarity:\", cosine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 125ms/step\n",
      "1/1 [==============================] - 0s 134ms/step\n"
     ]
    }
   ],
   "source": [
    "#predicting\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "# load an image from file\n",
    "image1 = load_img('New folder/test/test3.png', target_size=(224, 224))\n",
    "image2 = load_img('New folder/test/test5.png', target_size=(224, 224))\n",
    "\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "image1 = img_to_array(image1)\n",
    "image2 = img_to_array(image2)\n",
    "\n",
    "# reshape data for the model\n",
    "image1 = image1.reshape((1, image1.shape[0], image1.shape[1], image1.shape[2]))\n",
    "image2 = image2.reshape((1, image2.shape[0], image2.shape[1], image2.shape[2]))\n",
    "\n",
    "\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "# prepare the image for the VGG model\n",
    "image1 = preprocess_input(image1)\n",
    "image2 = preprocess_input(image2)\n",
    "\n",
    "# predict the probability across all output classes\n",
    "yhat1_base = basemodel.predict(image1)\n",
    "yhat2_base = basemodel.predict(image2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity: 0.7491024\n"
     ]
    }
   ],
   "source": [
    "A = yhat1_base[0]\n",
    "B = yhat2_base[0]\n",
    "\n",
    "cosine = np.dot(A,B)/(norm(A)*norm(B))\n",
    "print(\"Cosine Similarity:\", cosine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 140ms/step\n",
      "1/1 [==============================] - 0s 133ms/step\n"
     ]
    }
   ],
   "source": [
    "#predicting\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "# load an image from file\n",
    "image1 = load_img('New folder/test/dove_body_wash_pink.jpg', target_size=(224, 224))\n",
    "image2 = load_img('New folder/test/tooth_paste.jpeg', target_size=(224, 224))\n",
    "\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "image1 = img_to_array(image1)\n",
    "image2 = img_to_array(image2)\n",
    "\n",
    "# reshape data for the model\n",
    "image1 = image1.reshape((1, image1.shape[0], image1.shape[1], image1.shape[2]))\n",
    "image2 = image2.reshape((1, image2.shape[0], image2.shape[1], image2.shape[2]))\n",
    "\n",
    "\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "# prepare the image for the VGG model\n",
    "image1 = preprocess_input(image1)\n",
    "image2 = preprocess_input(image2)\n",
    "\n",
    "# predict the probability across all output classes\n",
    "yhat1_base = basemodel.predict(image1)\n",
    "yhat2_base = basemodel.predict(image2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity: 0.6270279\n"
     ]
    }
   ],
   "source": [
    "A = yhat1_base[0]\n",
    "B = yhat2_base[0]\n",
    "\n",
    "cosine = np.dot(A,B)/(norm(A)*norm(B))\n",
    "print(\"Cosine Similarity:\", cosine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a90aeebcf29d64a654773811cc170cb25061cb2498f10ac689db374c7bf325de"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
